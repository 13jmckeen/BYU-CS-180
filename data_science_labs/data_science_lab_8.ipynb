{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/Michael-Holland-Dev/CS180/blob/main/data_science_labs/data_science_lab_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "RvidLkZCc4A2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BYU CS 180 Lab 8"
      ],
      "metadata": {
        "id": "k0NimIjkdTzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction:\n",
        "For this assignment, you will build a simple univariate linear regression model to predict profits for a food truck."
      ],
      "metadata": {
        "id": "gHbfWVe-dMYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the data that you will use by running this command, and read it in using:"
      ],
      "metadata": {
        "id": "7NewZD37dbrM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbrM8d_3b91Y",
        "outputId": "44631617-aa50-48bb-cff7-b0d0d27c84b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-31 23:48:53--  https://raw.githubusercontent.com/porterjenkins/cs180-intro-data-science/master/data/food-truck.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1359 (1.3K) [text/plain]\n",
            "Saving to: ‚Äòfood-truck.csv.1‚Äô\n",
            "\n",
            "\rfood-truck.csv.1      0%[                    ]       0  --.-KB/s               \rfood-truck.csv.1    100%[===================>]   1.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-31 23:48:53 (21.0 MB/s) - ‚Äòfood-truck.csv.1‚Äô saved [1359/1359]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "!wget https://raw.githubusercontent.com/porterjenkins/cs180-intro-data-science/master/data/food-truck.csv\n",
        "data = pd.read_csv(\"food-truck.csv\", header=None, names=['X', 'Y'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will restrict ourselves to a linear hypothesis space, constructing a model that adheres to the following form:\n",
        "$$ f _\\Theta(x) = \\theta _0 + \\theta _1x $$\n",
        "\n",
        "You might notice that this equation is similar to the linear equation: $$ f(x) = b + mx $$\n",
        "\n",
        "(Yes you did use y=mx+b after 8th grade üòú)\n",
        "\n",
        "\n",
        "In this lab, you will be writing a machine learning model that learns/approximates 2 parameters. The first one is $\\theta_0$ which represents learning *b* (the bias or the intercept. The second one is $\\theta_1$ which represents learning *m* (the weight, or the slope). This type of machine learning model is traditionally called Least Squares"
      ],
      "metadata": {
        "id": "h5eqHpQ9yx7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given data, your goal will be to estimate the parameters of this model using the method of steepest gradient descent. The parameters are defined within the construct:\n",
        "$$ \\theta_p = \\{\\theta_0,\\theta_1\\} $$"
      ],
      "metadata": {
        "id": "jDiIwcCh1Owd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "which is the vector of learnable coefficients that weight the observed variables, and where $\\theta_1$ is a single bias coefficient. We can learn these parameters by minimizing average squared error. Thus, the loss function you will want to implement is:\n",
        "\n",
        "Equation 1: $$ ùìõ(\\Theta) = \\frac{1}{2m}\\sum_{i=1}^m (f(x^i)-y^i)^2 $$\n",
        "\n",
        "This equation may look very scary at first, but it's really not that scary, so let's break it down, and define our variables.\n",
        "*   m is the number of datapoints in the data set.\n",
        "*   i is the index of the data point tuple that we're looking at in the sum.\n",
        "*   ùìõ is the loss function (think of it like f(x)\n",
        "*   $\\Theta$ is the list of parameters to estimate\n",
        "*   $(f(x^i)-y^i)^2$ is the squared difference between the predicted output, and the actual output\n",
        "\n",
        "So wrapping it all together, the loss function is taking the sum of the squared differences, and dividing it by 2 * the number of datapoints in the dataset.\n",
        "\n",
        "The goal of this lab is to minimize the loss function, because that means that our predicted values and the actual values are getting closer together. (Quick thought exercise, ask yourself why is it good that our predicted and actual values are getting closer together) \n",
        "\n"
      ],
      "metadata": {
        "id": "jc-wWrUc12NG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Partial Derivatives"
      ],
      "metadata": {
        "id": "LzsatAR4GGUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3UOHtUJNJUTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analytically derive the gradient of the loss function with respect to the model parameters, $\\theta_0$ and $\\theta_1$. (take the partial derivative with respect to the given parameters$ \n",
        "\n",
        "Hint: You will need to find these two derivatives in order to calculate the gradient (L is just a less fancy version of the L(Œ∏) above):\n",
        "\n"
      ],
      "metadata": {
        "id": "5zGn1zlbGK56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Give the partial derivative for the parameter $\\theta_0$ below:"
      ],
      "metadata": {
        "id": "ye_vjnFKGiR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Enter Answer Here)"
      ],
      "metadata": {
        "id": "Csf84DuKGw_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Give the partial derivative for the parameter $\\theta_1$ below:"
      ],
      "metadata": {
        "id": "4_Dg07RwGrgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Enter Answer Here)"
      ],
      "metadata": {
        "id": "jvD2WWiWKid0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the two equations above, fill out the gradient calculation function below:"
      ],
      "metadata": {
        "id": "C82MvdtJK-si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_gradient(X,y, theta):\n",
        "  # TODO: The gradient with respect to the bias (the y intercept)\n",
        "  dL_d0 = \n",
        "  # TODO: The gradient with respect to the weight (the slope)\n",
        "  dL_d1 = \n",
        "  #nabla represents the full gradient, or a vector of the partial derivatives.\n",
        "  nabla = (dL_d0, dL_d1)\n",
        "  return nabla"
      ],
      "metadata": {
        "id": "INIE_kqpK7sm"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will update the current parameters with the function:\n",
        "\n",
        "Equation 2:\n",
        "$$\\theta_n = \\theta_p - Œ± \\frac{dùìõ(\\Theta)}{d\\theta_p}$$\n",
        "\n",
        "Again, there are some scary letters, but that's not to worry, I'm going to break them down for you below.\n",
        "\n",
        "* $\\theta_n$ is the updated parameter tuple.\n",
        "* $\\theta_p$ is the previous parameter tuple.\n",
        "* Œ± is the learning rate (The rate at which the parameters are learning (I recommend a small value for this, like 0.1, 0.01, or 0.001) Once the lab is finished you should try different values for your learning rate and see how it changes the convergence of the parameters that minimize the loss.\n",
        "* $\\frac{dùìõ(\\Theta)}{d\\theta_p}$ is the gradient (vector of partial derviatives) that you'll be updating the parameters\n",
        "\n",
        "Putting it all together, we have the previous parameters being subtracted from the gradient multiplied by a learning rate.\n",
        "\n",
        "Attempt to explain why/how the update rule uses the loss function to converge the parameters to make the loss function approach it's minimum state (Credit will be given at an attempt of an explanation, not based on correctness): "
      ],
      "metadata": {
        "id": "JUtKOGZOGxqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Enter Answer Here)"
      ],
      "metadata": {
        "id": "sdCxSZAcKsS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: The Dark Descent (Compute Cost)"
      ],
      "metadata": {
        "id": "LiYPG03jKvWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to correctly recreate the gradient descent algorithm, you need to compute the cost function. Use the equation given in the introduction to fill out the functions below to correctly produce the right loss."
      ],
      "metadata": {
        "id": "Gp6q-MueNuPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, theta):\n",
        "  # TODO: Write the code to output the predictions of the input \n",
        "  # Hint: You'll use all of the arguments of the function, and remember that theta is a tuple\n",
        "  predictions = \n",
        "  return predictions\n",
        "\n",
        "def sum_mean_squared_error(y, y_hat):\n",
        "  # TODO: Write the sum of the mean squared error.\n",
        "  # Hint: Follow equation \n",
        "  mean_squared_error = \n",
        "  return mean_squared_error\n",
        "\n",
        "def calculate_loss(X, y, theta):\n",
        "\t# TODO: Write your compute loss function below.\n",
        "  # Hint: You'll use the predict and the sum_mean_squared_error functions defined above\n",
        "  y_hat = \n",
        "  loss = \n",
        "  return loss"
      ],
      "metadata": {
        "id": "vVDwIWcJOegx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain to me like I'm five what the code above is doing:"
      ],
      "metadata": {
        "id": "xAUhDKl1QkxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Enter Answer Here)"
      ],
      "metadata": {
        "id": "Kc2Dc_tzQsLz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Hold The Line (Training Your Least Squared Algorithm)"
      ],
      "metadata": {
        "id": "x3P97BfsQ3JB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### There are a couple of hyperparameters that you should know about before training your algorithm:\n",
        "\n",
        "#### Learning Rate / Step Size:\n",
        "Learning rate (traditionally denoted as Œ± or alpha) is the rate at which your algorithm learns. You can also think of it as the step size in which the weight and bias ($\\theta_0$ and $\\theta_1$) are being updated. If the step size is too large, the weights are updated, and the loss value will blow up and never converge. If the learning rate is too small, the algorithm will take forever to converge, and won't be an effective use of your time. If you have a smaller learning rate, I recommend using a higher epoch count.\n",
        "\n",
        "#### Epochs / Iterations:\n",
        "Epochs is the number of times that the algorithm runs through the dataset. For example, if the epoch count is 500, then the algorithm with run through the dataset 500 times.\n",
        "\n",
        "#### Batch Size:\n",
        "You won't use batch size for this lab, but I still feel like it's an important concept to learn. Batch size is splitting the dataset up into smaller chunks and then feeding those smaller chuncks of data into the ML model. When it comes to larger models such as Neural Networks, it's important to have the right batch size otherwise your computer may run out of memory.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IY0BRQMpRlC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What you need to do:\n",
        "- [ ] Finish writing the training loop to train your least squares model\n",
        "- [ ] Plot your linear model after 5 epochs\n",
        "- [ ] Plot your linear model after 100 epochs\n",
        "- [ ] Plot your linear model after 1000 epochs\n",
        "- [ ] Plot your linear model after 10,000 epochs\n",
        "- [ ] Plot the relationship between the epoch number and the loss at that given epoch"
      ],
      "metadata": {
        "id": "_qDd0v1YUJ9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below you need to fill out the TODO parts of the training function"
      ],
      "metadata": {
        "id": "Oxx63wSgZVLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(dataframe, epochs, learning_rate):\n",
        "  # Get the X and y values of the column.\n",
        "  cols = dataframe.shape[1]\n",
        "  X = dataframe.iloc[:,0:cols-1]\n",
        "  y = dataframe.iloc[:,cols-1:cols]\n",
        "\n",
        "  # Turn the X and y values into numpy arrays.\n",
        "  X = np.array(X.values)\n",
        "  y = np.array(y.values)\n",
        "\n",
        "  # convert to numpy arrays and initalize the parameter array theta\n",
        "  w = np.zeros((1,X.shape[1]))\n",
        "  b = np.array([0])\n",
        "  theta = (b, w)\n",
        "\n",
        "  # TODO: calculate the initial cost\n",
        "  initial_L = \n",
        "\n",
        "  i = 0\n",
        "\n",
        "  # Initialize the loss values for the plot.\n",
        "  loss_values = []\n",
        "  loss_values.append(initial_L)\n",
        "\n",
        "  while i < epochs:\n",
        "    # TODO: Calculate Gradient\n",
        "    dL_db, dL_dw = \n",
        "    t_0 = theta[0]\n",
        "    t_1 = theta[1]\n",
        "\n",
        "    # TODO: update theta with respect to the calculated gradient\n",
        "    # Hint: \n",
        "    updated_t_0 = \n",
        "    updated_t_1 = \n",
        "\n",
        "    theta = (updated_t_0, updated_t_1)\n",
        "\n",
        "    # TODO: Calculated new loss using the updated theta values, and add it to the loss_values list.\n",
        "    L = \n",
        "\n",
        "    #update I\n",
        "    i += 1\n",
        "  return loss_values, theta\n"
      ],
      "metadata": {
        "id": "7EFH9H1cRhSX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_line(dataframe, theta, epoch, learning_rate):\n",
        "  # This function will plot your model based on some sampled values. \n",
        "  # There is no need to change this function, you'll just need to call it.\n",
        "  cols = data.shape[1]\n",
        "  X = dataframe.iloc[:,0:cols-1]\n",
        "  y = dataframe.iloc[:,cols-1:cols]\n",
        "  X = np.array(X.values)\n",
        "  y = np.array(y.values)\n",
        "\n",
        "  kludge = 0.25\n",
        "  X_test = np.linspace(data.X.min(), data.X.max(), 100)\n",
        "  X_test = np.expand_dims(X_test, axis=1)\n",
        "  \n",
        "  plt.plot(X_test, predict(X_test, theta), label=\"Model\")\n",
        "  plt.title(f\"Value for {epoch} epochs and {learning_rate} step\")\n",
        "  plt.scatter(X[:,0], y, edgecolor='g', s=20, label=\"Samples\")\n",
        "  plt.xlabel(\"x\")\n",
        "  plt.ylabel(\"y\")\n",
        "  plt.xlim((np.amin(X_test) - kludge, np.amax(X_test) + kludge))\n",
        "  plt.ylim((np.amin(y) - kludge, np.amax(y) + kludge))\n",
        "  plt.legend(loc=\"best\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "_sv8tD_aYaPK"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the two functions above to do plot the models after 5, 100, 1000, and 10000 epochs."
      ],
      "metadata": {
        "id": "oYajIvGMZr8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write the code for the individual iterations:\n",
        "# 5 Epochs\n",
        "\n",
        "# 100 Epochs\n",
        "\n",
        "# 1000 Epochs\n",
        "\n",
        "# 10000 Epochs"
      ],
      "metadata": {
        "id": "IsijuvK4ZSCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the 10,000 Epochs, plot the loss values with respect to epochs using a line graph."
      ],
      "metadata": {
        "id": "9UuayB93aVOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H7d77l9HaTEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4:"
      ],
      "metadata": {
        "id": "Ke029wcPasAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since learning a linear model is a convex optimization problem, you should see convergence to a low mean squared error. However, you will need to tune the learning rate/step size, Œ±  (bear in mind that values that are too big will result in divergence). You should record the settings you tried and what you found that worked also in the answer sheet. \n",
        "\n",
        "Write a few sentences describing what you learned from the training/model fitting process. \n",
        "\n",
        "Things to discuss: What happens when you change the step-size Œ±? How many epochs did you need to converge to a reasonable solution (for any given step size)?"
      ],
      "metadata": {
        "id": "3yT8gXHhau04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Enter Answer Here)"
      ],
      "metadata": {
        "id": "tKUndHnBa1C0"
      }
    }
  ]
}